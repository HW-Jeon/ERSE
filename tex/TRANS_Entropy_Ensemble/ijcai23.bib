@inproceedings{NIPS2013_1cecc7a7,
 author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Translating Embeddings for Modeling Multi-relational Data},
 url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
 volume = {26},
 year = {2013}
}

@article{10.1093/bib/bbac481,
    author = {Rivas-Barragan, Daniel and Domingo-Fernández, Daniel and Gadiya, Yojana and Healey, David},
    title = "{Ensembles of knowledge graph embedding models improve predictions for drug discovery}",
    journal = {Briefings in Bioinformatics},
    volume = {23},
    number = {6},
    pages = {bbac481},
    year = {2022},
    month = {11},
    abstract = "{Recent advances in Knowledge Graphs (KGs) and Knowledge Graph Embedding Models (KGEMs) have led to their adoption in a broad range of fields and applications. The current publishing system in machine learning requires newly introduced KGEMs to achieve state-of-the-art performance, surpassing at least one benchmark in order to be published. Despite this, dozens of novel architectures are published every year, making it challenging for users, even within the field, to deduce the most suitable configuration for a given application. A typical biomedical application of KGEMs is drug–disease prediction in the context of drug discovery, in which a KGEM is trained to predict triples linking drugs and diseases. These predictions can be later tested in clinical trials following extensive experimental validation. However, given the infeasibility of evaluating each of these predictions and that only a minimal number of candidates can be experimentally tested, models that yield higher precision on the top prioritized triples are preferred. In this paper, we apply the concept of ensemble learning on KGEMs for drug discovery to assess whether combining the predictions of several models can lead to an overall improvement in predictive performance. First, we trained and benchmarked 10 KGEMs to predict drug–disease triples on two independent biomedical KGs designed for drug discovery. Following, we applied different ensemble methods that aggregate the predictions of these models by leveraging the distribution or the position of the predicted triple scores. We then demonstrate how the ensemble models can achieve better results than the original KGEMs by benchmarking the precision (i.e., number of true positives prioritized) of their top predictions. Lastly, we released the source code presented in this work at https://github.com/enveda/kgem-ensembles-in-drug-discovery.}",
    issn = {1477-4054},
    doi = {10.1093/bib/bbac481},
    url = {https://doi.org/10.1093/bib/bbac481},
    eprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac481/47144690/supplementary\_file\_bbac481.pdf},
}




@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@book{zhou2012ensemble,
  title={Ensemble methods: foundations and algorithms},
  author={Zhou, Zhi-Hua},
  year={2012},
  publisher={CRC press}
}

@inproceedings{9533372,
  author={Xu, Chengjin and Nayyeri, Mojtaba and Vahdati, Sahar and Lehmann, Jens},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multiple Run Ensemble Learning with Low-Dimensional Knowledge Graph Embeddings}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN52387.2021.9533372}}


@inproceedings{freund1996experiments,
  title={Experiments with a new boosting algorithm},
  author={Freund, Yoav and Schapire, Robert E and others},
  booktitle={icml},
  volume={96},
  pages={148--156},
  year={1996},
  organization={Citeseer}
}

@article{choi2020approach,
  title={An approach to knowledge base completion by a committee-based knowledge graph embedding},
  author={Choi, Su Jeong and Song, Hyun-Je and Park, Seong-Bae},
  journal={Applied Sciences},
  volume={10},
  number={8},
  pages={2651},
  year={2020},
  publisher={MDPI}
}

@article{WANG20221041,
title = {A probabilistic ensemble approach for knowledge graph embedding},
journal = {Neurocomputing},
volume = {500},
pages = {1041-1051},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222007494},
author = {Yinquan Wang and Yao Chen and Zhe Zhang and Tian Wang},
keywords = {Knowledge graph embedding, Ensemble learning},
abstract = {Knowledge graph embedding (KGE) is a technique for embedding entities and relations of knowledge graphs (KGs) into continuous vector spaces while maintaining the inherent structure of KGs, in this way link prediction can be facilitated by scoring candidate triples. Existing KGE models only capture specific features of a KG, however, link prediction capacity heavily relies on comprehensive features of a KG. In this work, we propose an ensemble approach for KGE by incorporating the features captured by different KGE models. As a key step, we propose a probabilistic scoring index for characterizing the link prediction performance of a given KGE model. Based on this we established a theoretical framework to calculate the optimal parameters for the ensemble model and predict its corresponding link prediction rate. In particular, we proved that our ensemble approach can always improve link prediction performance under some assumptions. The Upper Confidence Bound Algorithm is then used to adjust the parameters. Experimental results on two widely used KGs show that the proposed ensemble approach achieves the state-of-the-art link prediction rate.}
}

@inproceedings{trouillon2016complex,
  title={Complex embeddings for simple link prediction},
  author={Trouillon, Th{\'e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, {\'E}ric and Bouchard, Guillaume},
  booktitle={International conference on machine learning},
  pages={2071--2080},
  year={2016},
  organization={PMLR}
}

@article{zheng2021knowledge,
  title={Knowledge base graph embedding module design for Visual question answering model},
  author={Zheng, Wenfeng and Yin, Lirong and Chen, Xiaobing and Ma, Zhiyang and Liu, Shan and Yang, Bo},
  journal={Pattern recognition},
  volume={120},
  pages={108153},
  year={2021},
  publisher={Elsevier}
}

@article{chen2021topic,
  title={Topic analysis and development in knowledge graph research: A bibliometric review on three decades},
  author={Chen, Xieling and Xie, Haoran and Li, Zongxi and Cheng, Gary},
  journal={Neurocomputing},
  volume={461},
  pages={497--515},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{krompass2015ensemble,
  title={Ensemble solutions for link-prediction in knowledge graphs},
  author={Krompa{\ss}, Denis and Tresp, Volker},
  booktitle={PKDD ECML 2nd Workshop on Linked Data for Knowledge Discovery},
  year={2015}
}

@article{sun2019rotate,
  title={Rotate: Knowledge graph embedding by relational rotation in complex space},
  author={Sun, Zhiqing and Deng, Zhi-Hong and Nie, Jian-Yun and Tang, Jian},
  journal={arXiv preprint arXiv:1902.10197},
  year={2019}
}

@article{osamor2021enhancing,
  title={Enhancing the weighted voting ensemble algorithm for tuberculosis predictive diagnosis},
  author={Osamor, Victor Chukwudi and Okezie, Adaugo Fiona},
  journal={Scientific Reports},
  volume={11},
  number={1},
  pages={14806},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@Article{a13010026,
AUTHOR = {Karlos, Stamatis and Kostopoulos, Georgios and Kotsiantis, Sotiris},
TITLE = {A Soft-Voting Ensemble Based Co-Training Scheme Using Static Selection for Binary Classification Problems},
JOURNAL = {Algorithms},
VOLUME = {13},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {26},
URL = {https://www.mdpi.com/1999-4893/13/1/26},
ISSN = {1999-4893},
ABSTRACT = {In recent years, a forward-looking subfield of machine learning has emerged with important applications in a variety of scientific fields. Semi-supervised learning is increasingly being recognized as a burgeoning area embracing a plethora of efficient methods and algorithms seeking to exploit a small pool of labeled examples together with a large pool of unlabeled ones in the most efficient way. Co-training is a representative semi-supervised classification algorithm originally based on the assumption that each example can be described by two distinct feature sets, usually referred to as views. Since such an assumption can hardly be met in real world problems, several variants of the co-training algorithm have been proposed dealing with the absence or existence of a naturally two-view feature split. In this context, a Static Selection Ensemble-based co-training scheme operating under a random feature split strategy is outlined regarding binary classification problems, where the type of the base ensemble learner is a soft-Voting one composed of two participants. Ensemble methods are commonly used to boost the predictive performance of learning models by using a set of different classifiers, while the Static Ensemble Selection approach seeks to find the most suitable structure of ensemble classifier based on a specific criterion through a pool of candidate classifiers. The efficacy of the proposed scheme is verified through several experiments on a plethora of benchmark datasets as statistically confirmed by the Friedman Aligned Ranks non-parametric test over the behavior of classification accuracy, F1-score, and Area Under Curve metrics.},
DOI = {10.3390/a13010026}
}

@article{rivas2022ensembles,
  title={Ensembles of knowledge graph embedding models improve predictions for drug discovery},
  author={Rivas-Barragan, Daniel and Domingo-Fern{\'a}ndez, Daniel and Gadiya, Yojana and Healey, David},
  journal={Briefings in Bioinformatics},
  volume={23},
  number={6},
  pages={bbac481},
  year={2022},
  publisher={Oxford University Press}
}

@article{ji2021survey,
  title={A survey on knowledge graphs: Representation, acquisition, and applications},
  author={Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={2},
  pages={494--514},
  year={2021},
  publisher={IEEE}
}

@article{yang2014embedding,
  title={Embedding entities and relations for learning and inference in knowledge bases},
  author={Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  journal={arXiv preprint arXiv:1412.6575},
  year={2014}
}

@inproceedings{ji2016knowledge,
  title={Knowledge graph completion with adaptive sparse transfer matrix},
  author={Ji, Guoliang and Liu, Kang and He, Shizhu and Zhao, Jun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{9533372,
  author={Xu, Chengjin and Nayyeri, Mojtaba and Vahdati, Sahar and Lehmann, Jens},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multiple Run Ensemble Learning with Low-Dimensional Knowledge Graph Embeddings}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN52387.2021.9533372}
}

@article{wissner2013causal,
  title={Causal entropic forces},
  author={Wissner-Gross, Alexander D and Freer, Cameron E},
  journal={Physical review letters},
  volume={110},
  number={16},
  pages={168702},
  year={2013},
  publisher={APS}
}

@article{de2005tutorial,
  title={A tutorial on the cross-entropy method},
  author={De Boer, Pieter-Tjerk and Kroese, Dirk P and Mannor, Shie and Rubinstein, Reuven Y},
  journal={Annals of operations research},
  volume={134},
  pages={19--67},
  year={2005},
  publisher={Springer}
}

@inproceedings{zhang-etal-2022-trans,
    title = "{T}ran{S}: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation",
    author = "Zhang, Xuanyu  and
      Yang, Qing  and
      Xu, Dongliang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.86",
    doi = "10.18653/v1/2022.findings-emnlp.86",
    pages = "1202--1208",
    abstract = "Knowledge graph embedding (KGE) aims to learn continuous vector representations of relations and entities in knowledge graph (KG). Recently, transition-based KGE methods have become popular and achieved promising performance. However, scoring patterns like TransE are not suitable for complex scenarios where the same entity pair has different relations. Although some models attempt to employ entity-relation interaction or projection to improve entity representation for one-to-many/many-to-one/many-to-many complex relations, they still continue the traditional scoring pattern, where only a single relation vector in the relation part is used to translate the head entity to the tail entity or their variants. And recent research shows that entity representation only needs to consider entities and their interactions to achieve better performance. Thus, in this paper, we propose a novel transition-based method, TranS, for KGE. The single relation vector of the relation part in the traditional scoring pattern is replaced by the synthetic relation representation with entity-relation interactions to solve these issues. And the entity part still retains its independence through entity-entity interactions. Experiments on a large KG dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results.",
}


@inproceedings{han-etal-2018-openke,
    title = "{O}pen{KE}: An Open Toolkit for Knowledge Embedding",
    author = "Han, Xu  and
      Cao, Shulin  and
      Lv, Xin  and
      Lin, Yankai  and
      Liu, Zhiyuan  and
      Sun, Maosong  and
      Li, Juanzi",
    editor = "Blanco, Eduardo  and
      Lu, Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2024",
    doi = "10.18653/v1/D18-2024",
    pages = "139--144",
    abstract = "We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes operational efficiency to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new models into the framework. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on \url{http://openke.thunlp.org/}.",
}


@inproceedings{toutanova-chen-2015-observed,
    title = "Observed versus latent features for knowledge base and text inference",
    author = "Toutanova, Kristina  and
      Chen, Danqi",
    editor = "Allauzen, Alexandre  and
      Grefenstette, Edward  and
      Hermann, Karl Moritz  and
      Larochelle, Hugo  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-4007",
    doi = "10.18653/v1/W15-4007",
    pages = "57--66",
}

@inproceedings{han2018openke,
    title={OpenKE: An Open Toolkit for Knowledge Embedding},
    author={Han, Xu and Cao, Shulin and Lv Xin and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Li, Juanzi},
    booktitle={Proceedings of EMNLP},
    year={2018}
}

@inproceedings{10.1145/1376616.1376746,
author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
year = {2008},
isbn = {9781605581026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1376616.1376746},
doi = {10.1145/1376616.1376746},
abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
booktitle = {Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data},
pages = {1247–1250},
numpages = {4},
keywords = {tuple store, semantic network, collaborative systems},
location = {Vancouver, Canada},
series = {SIGMOD '08}
}

@inproceedings{renyi1961measures,
  title={On measures of entropy and information},
  author={R{\'e}nyi, Alfr{\'e}d},
  booktitle={Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  volume={4},
  pages={547--562},
  year={1961},
  organization={University of California Press}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{berkson1944application,
  title={Application of the logistic function to bio-assay},
  author={Berkson, Joseph},
  journal={Journal of the American statistical association},
  volume={39},
  number={227},
  pages={357--365},
  year={1944},
  publisher={JSTOR}
}

@article{burr1942cumulative,
  title={Cumulative frequency functions},
  author={Burr, Irving W},
  journal={The Annals of mathematical statistics},
  volume={13},
  number={2},
  pages={215--232},
  year={1942},
  publisher={JSTOR}
}

@article{bordes2013translating,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{wang2014knowledge,
  title={Knowledge graph embedding by translating on hyperplanes},
  author={Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={28},
  number={1},
  year={2014}
}

@inproceedings{lin2015learning,
  title={Learning entity and relation embeddings for knowledge graph completion},
  author={Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={29},
  number={1},
  year={2015}
}

@inproceedings{ji2015knowledge,
  title={Knowledge graph embedding via dynamic mapping matrix},
  author={Ji, Guoliang and He, Shizhu and Xu, Liheng and Liu, Kang and Zhao, Jun},
  booktitle={Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers)},
  pages={687--696},
  year={2015}
}

@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}
